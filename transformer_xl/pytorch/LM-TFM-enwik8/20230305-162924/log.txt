====================================================================================================
    - data : ../../data/ctl/
    - dataset : enwik8
    - n_layer : 4
    - n_head : 4
    - d_head : 32
    - d_embed : 128
    - d_model : 128
    - d_inner : 512
    - dropout : 0.1
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.01
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 100000
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - clip_nonemb : False
    - max_step : 400000
    - batch_size : 22
    - batch_chunk : 1
    - tgt_len : 512
    - eval_tgt_len : 50
    - ext_len : 0
    - mem_len : 0
    - not_tied : True
    - seed : 1111
    - cuda : True
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : False
    - log_interval : 200
    - eval_interval : 1000
    - work_dir : LM-TFM-enwik8/20230305-162924
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 0
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : -1
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - tied : False
    - n_token : 181
    - n_all_param : 903349
    - n_nonemb_param : 856576
====================================================================================================
#params = 903349
#non emb params = 856576
| epoch  12 step      200 |      2 batches | lr 2e-05 | ms/batch 367.15 | loss  4.90 | bpc   7.06795
| epoch  23 step      400 |      4 batches | lr 4e-05 | ms/batch 364.53 | loss  4.39 | bpc   6.33653
| epoch  34 step      600 |      6 batches | lr 6e-05 | ms/batch 364.32 | loss  3.99 | bpc   5.75889
| epoch  45 step      800 |      8 batches | lr 8e-05 | ms/batch 364.26 | loss  3.78 | bpc   5.45845
| epoch  56 step     1000 |     10 batches | lr 0.0001 | ms/batch 364.50 | loss  3.72 | bpc   5.36108
----------------------------------------------------------------------------------------------------
| Eval   1 at step     1000 | time: 365.00s | valid loss  3.69 | bpc   5.31912
----------------------------------------------------------------------------------------------------
| epoch  67 step     1200 |     12 batches | lr 0.00012 | ms/batch 364.91 | loss  3.69 | bpc   5.32031
| epoch  78 step     1400 |     14 batches | lr 0.00014 | ms/batch 364.49 | loss  3.67 | bpc   5.29788
| epoch  89 step     1600 |     16 batches | lr 0.00016 | ms/batch 364.20 | loss  3.66 | bpc   5.28298
----------------------------------------------------------------------------------------------------
Exiting from training early
====================================================================================================
| End of training | test loss  3.70 | test bpc   5.33805
====================================================================================================
