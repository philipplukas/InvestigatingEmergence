====================================================================================================
    - data : ../../data/ctl/
    - dataset : enwik8
    - n_layer : 4
    - n_head : 4
    - d_head : 32
    - d_embed : 128
    - d_model : 128
    - d_inner : 512
    - dropout : 0.1
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.1
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - clip_nonemb : False
    - max_step : 400000
    - batch_size : 22
    - batch_chunk : 1
    - tgt_len : 32
    - eval_tgt_len : 32
    - ext_len : 0
    - mem_len : 64
    - not_tied : False
    - seed : 1111
    - cuda : True
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : False
    - log_interval : 200
    - eval_interval : 1000
    - work_dir : LM-TFM-enwik8/20230305-161410
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 0
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : -1
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - tied : True
    - n_token : 181
    - n_all_param : 880181
    - n_nonemb_param : 856576
====================================================================================================
#params = 880181
#non emb params = 856576
| epoch   1 step      200 |    200 batches | lr 0.1 | ms/batch 25.40 | loss  5.10 | bpc   7.35117
| epoch   2 step      400 |    115 batches | lr 0.1 | ms/batch 20.06 | loss  4.97 | bpc   7.17430
| epoch   3 step      600 |     30 batches | lr 0.1 | ms/batch 18.27 | loss  4.98 | bpc   7.18233
| epoch   3 step      800 |    230 batches | lr 0.1 | ms/batch 18.71 | loss  4.99 | bpc   7.19211
| epoch   4 step     1000 |    145 batches | lr 0.1 | ms/batch 22.98 | loss  4.98 | bpc   7.18583
----------------------------------------------------------------------------------------------------
| Eval   1 at step     1000 | time: 21.16s | valid loss  4.95 | bpc   7.14363
----------------------------------------------------------------------------------------------------
| epoch   5 step     1200 |     60 batches | lr 0.1 | ms/batch 19.15 | loss  4.99 | bpc   7.19187
| epoch   5 step     1400 |    260 batches | lr 0.1 | ms/batch 18.65 | loss  4.99 | bpc   7.20033
| epoch   6 step     1600 |    175 batches | lr 0.1 | ms/batch 24.36 | loss  4.99 | bpc   7.19654
| epoch   7 step     1800 |     90 batches | lr 0.1 | ms/batch 18.73 | loss  4.99 | bpc   7.19841
| epoch   8 step     2000 |      5 batches | lr 0.1 | ms/batch 18.69 | loss  4.99 | bpc   7.20029
----------------------------------------------------------------------------------------------------
| Eval   2 at step     2000 | time: 19.87s | valid loss  5.01 | bpc   7.22508
----------------------------------------------------------------------------------------------------
| epoch   8 step     2200 |    205 batches | lr 0.1 | ms/batch 21.84 | loss  4.99 | bpc   7.20026
| epoch   9 step     2400 |    120 batches | lr 0.1 | ms/batch 20.74 | loss  4.99 | bpc   7.20085
| epoch  10 step     2600 |     35 batches | lr 0.1 | ms/batch 18.31 | loss  4.99 | bpc   7.19804
| epoch  10 step     2800 |    235 batches | lr 0.1 | ms/batch 18.26 | loss  5.00 | bpc   7.21179
| epoch  11 step     3000 |    150 batches | lr 0.1 | ms/batch 23.85 | loss  4.99 | bpc   7.19979
----------------------------------------------------------------------------------------------------
| Eval   3 at step     3000 | time: 20.61s | valid loss  4.96 | bpc   7.15764
----------------------------------------------------------------------------------------------------
| epoch  12 step     3200 |     65 batches | lr 0.1 | ms/batch 19.00 | loss  4.99 | bpc   7.20011
| epoch  12 step     3400 |    265 batches | lr 0.1 | ms/batch 18.35 | loss  5.00 | bpc   7.20677
| epoch  13 step     3600 |    180 batches | lr 0.1 | ms/batch 23.04 | loss  5.00 | bpc   7.20821
| epoch  14 step     3800 |     95 batches | lr 0.1 | ms/batch 18.58 | loss  4.99 | bpc   7.20416
| epoch  15 step     4000 |     10 batches | lr 0.1 | ms/batch 18.30 | loss  5.00 | bpc   7.20892
----------------------------------------------------------------------------------------------------
| Eval   4 at step     4000 | time: 19.46s | valid loss  4.97 | bpc   7.16845
----------------------------------------------------------------------------------------------------
| epoch  15 step     4200 |    210 batches | lr 0.1 | ms/batch 20.16 | loss  5.00 | bpc   7.21237
| epoch  16 step     4400 |    125 batches | lr 0.1 | ms/batch 21.88 | loss  4.99 | bpc   7.20257
| epoch  17 step     4600 |     40 batches | lr 0.1 | ms/batch 18.34 | loss  5.00 | bpc   7.20851
| epoch  17 step     4800 |    240 batches | lr 0.1 | ms/batch 18.48 | loss  5.00 | bpc   7.21399
| epoch  18 step     5000 |    155 batches | lr 0.1 | ms/batch 23.60 | loss  4.99 | bpc   7.20368
----------------------------------------------------------------------------------------------------
| Eval   5 at step     5000 | time: 20.50s | valid loss  4.99 | bpc   7.20046
----------------------------------------------------------------------------------------------------
| epoch  19 step     5200 |     70 batches | lr 0.1 | ms/batch 18.36 | loss  5.00 | bpc   7.21410
| epoch  19 step     5400 |    270 batches | lr 0.1 | ms/batch 18.17 | loss  5.00 | bpc   7.21378
| epoch  20 step     5600 |    185 batches | lr 0.1 | ms/batch 21.30 | loss  5.00 | bpc   7.20706
| epoch  21 step     5800 |    100 batches | lr 0.0999 | ms/batch 20.56 | loss  5.00 | bpc   7.20847
| epoch  22 step     6000 |     15 batches | lr 0.0999 | ms/batch 18.70 | loss  5.00 | bpc   7.21398
----------------------------------------------------------------------------------------------------
| Eval   6 at step     6000 | time: 19.43s | valid loss  4.98 | bpc   7.18387
----------------------------------------------------------------------------------------------------
| epoch  22 step     6200 |    215 batches | lr 0.0999 | ms/batch 19.60 | loss  5.00 | bpc   7.21780
| epoch  23 step     6400 |    130 batches | lr 0.0999 | ms/batch 23.38 | loss  5.00 | bpc   7.21337
| epoch  24 step     6600 |     45 batches | lr 0.0999 | ms/batch 18.42 | loss  5.00 | bpc   7.21438
| epoch  24 step     6800 |    245 batches | lr 0.0999 | ms/batch 18.32 | loss  5.00 | bpc   7.21341
| epoch  25 step     7000 |    160 batches | lr 0.0999 | ms/batch 23.36 | loss  5.00 | bpc   7.21017
----------------------------------------------------------------------------------------------------
| Eval   7 at step     7000 | time: 20.63s | valid loss  4.98 | bpc   7.18085
----------------------------------------------------------------------------------------------------
| epoch  26 step     7200 |     75 batches | lr 0.0999 | ms/batch 18.88 | loss  5.00 | bpc   7.21549
| epoch  26 step     7400 |    275 batches | lr 0.0999 | ms/batch 18.22 | loss  5.00 | bpc   7.21123
| epoch  27 step     7600 |    190 batches | lr 0.0999 | ms/batch 21.62 | loss  5.00 | bpc   7.21378
| epoch  28 step     7800 |    105 batches | lr 0.0999 | ms/batch 21.81 | loss  5.00 | bpc   7.21667
----------------------------------------------------------------------------------------------------
Exiting from training early
====================================================================================================
| End of training | test loss  4.97 | test bpc   7.17426
====================================================================================================
